Practical Machine Learning Assignment
=====================================
## Background
In this project, we tried to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants that were asked to perform barbell lifts correctly and incorrectly in 5 different ways to determine whether personal fitness tracking devices could be used to determine the form (i.e. how well the exercise is done) of an exerciser. This dataset is originally from *http://groupware.les.inf.puc-rio.br/har*, and was obtained from the Coursera website. We used a Random Forest model because of its known performance abilities in general. We find that we can predict rather accurately (~99% accuracy in a test/validation sample) barbell lifting form based on fitness tracking devices.

## Exploratory analysis and data pre-processing
```{r, echo=FALSE}
setwd('/Users/benayoun/Dropbox/Coursera_data_science/practical_machine_learning/')
```

First, we load the caret package and read in memory the training dataset.
```{r, cache=TRUE}
library('caret')

# read file in
har.training <- read.csv('pml-training.csv',header=TRUE)

dim(har.training)
#summary(har.training)
```

There are 160 variables and 19622 measurements. However, looking at a summary of the data (here commented out for report brevity), we notice that a number of variables seem to have more than 19000 NA values, either depicted as "NA", the empty string "" or "#DIV/0!" which need to be excluded, as then can't be too informative in the model building.

```{r, cache=TRUE}
notUsable <- rep(FALSE,160)
for (i in 1:160) {
  if (sum(har.training[,i] %in% NA) > 19000) {
    notUsable[i] <- TRUE
  }
  if (sum(har.training[,i] %in% "") > 19000) {
    notUsable[i] <- TRUE
  }
  if (sum(har.training[,i] %in% "#DIV/0!") > 19000) {
    notUsable[i] <- TRUE
  }
}
sum(notUsable)
```

Let's now exlude the 100 NA-rich variables, as well as the first five columns, which should not be used for the model because they would be irrelevant to our question: these seem to be linked to time of day and date where the measurements were taken (i.e. **timestamp** values and window values), name of exerciser and the number of the measurement.
```{r, cache=TRUE}
clean.training <- data.frame(har.training[,-c(which(notUsable))])
colnames(clean.training)

clean.training <- clean.training[,-(1:7)]
```

We have 53 variables left: 52 potential high-quality predictors and 1 output variable ("classe"). Now, we examine the independance or correlation of the remaining variables to estimate how much information would be provided for model construction.
```{r, cache=TRUE, fig.width=8, fig.height = 8,}
library('pheatmap')
pheatmap(cor(clean.training[,-53]+0))
```

We see that some of the variables are very correlated (e.g. 'gyros_forearm_y' and 'gyros_dumbell_z'), but we may increase bias in the model by dropping them. Including them may increase variance, we will keep them to avoid increasing the bias. We also decide against using a PCA transformation of the variables to consolidate correlated vairables, which would decrease the interpretability of the model.

## Model fitting
Here, we decide to use Random Forests to learn a classifier of barbell form, because this algorithm is one of the best performers that are still interpretable (i.e. we can see which variables help the model make decisions). Random Forests are also more robust to correlated variables. To ensure a robust choice of model parameters and reasonnable computing time, we use 5-fold cross validation.
```{r, fig.width=8, fig.height = 2, cache=TRUE}
# split data into training and testing/validation sets
final.training <- createDataPartition(clean.training$classe, p=0.75, list=FALSE)
har.training <- clean.training[final.training,]
har.testing <- clean.training[-final.training,]

# set a seed for the random number generator for reproducibility of numerical results
set.seed(123456)

# use 5-fold cross-validation to build the model
ctrl.opt <- trainControl(method = "cv", number = 5)

# train model with caret train function
rf.fit <- train(har.training$classe ~ ., method="rf",data=har.training,importance=TRUE,trControl = ctrl.opt)
```

Now, let's examine the model built by this call.
```{r}
rf.fit$finalModel
```

```{r,echo = FALSE}
err <- 100 * rf.fit$finalModel$err.rate[500]
acc <- 100 - err
```

We had `r rf.fit$finalModel$ntree` trees in the model. The random forest produces an OOB estimate of error rate of `r err` %, corresponding to an OOB accuracy of `r acc`%. However, to get an estimate of the **real** error of the model, we use the testing/validation sample that we obtained by partitionning the data pre-training of the model, and whose real class labels are known.

```{r}
# predict on the partionned testing/validation data
rf.preds <- predict(rf.fit, har.testing)
confus.mat <- confusionMatrix(har.testing$classe, rf.preds)
confus.mat
```

```{r,echo = FALSE}
final.acc <- 100*confus.mat$overall[1]
```

Using data that was not used to build the model, we find an out-of-sample accuracy of `r final.acc`%. Thus it seems that we have built a high performance model, with high accuracy that is validated even using data that was not included in the training phase.

We can also examine variable importance to see which variables contributed most to the final model:
```{r,echo = TRUE}
varImpPlot(rf.fit$finalModel, sort = TRUE, type = 1, pch = 16, bg = "red", cex = 1, main = "Variable importance in model")
```

## Prediction on the unknown testing data
Now that we know that our model performs reasonnably well, we can use it to predict the class labels of the unknown samples provided in the assignment: we use our model on the provided 'test' dataset, which is processed similarly to our training set, and output the required files.

```{r, cache=TRUE}
har.testing <- read.csv('pml-testing.csv',header=TRUE)

# Select the same variables than in the training phase
clean.testing <- data.frame(har.testing[,-c(which(notUsable))])
clean.testing <- clean.testing[,-(1:7)]

# run predictions
test.preds <- as.character(predict(rf.fit, clean.testing))
# we don't show the results of these predictions here to adhere with the Coursera code of honor.
```

## Conclusion
We used Random Forests on accelerometer data to attempt to build a predictive model of whether wearers were using good form or not while lifting a barbell. Our model performed well in term of OOB accuracy, but also according to an out-of-sample validation set whose class labels (i.e. form while lifting the barbell) were already know. Finally, it seems that the model performs rather well also on unlabeled data in a 3rd independent dataset provided for prediction, since according to the automatic grading, the predicted labels were all correct.
